<h2>Generative Deep Learning with Multiple modalities</h2>

This project explores <b>Generative Deep Learning</b> techniques to inter-relate data from multiple modelities i.e. text, image, speech etc. The projecct makes use of a State-of-The-Art <em>(SOTA)</em> generative <em>Deep Learning architectures</em> which are arranged in a complementary fashion kept in a feedback loop.

This project makes use of the following <b><em>SOTA</em></b> Deep-Learning architectures to generatively convert text-prompts / sentences into images:

- OpenAI's Contrastive Language–Image Pre-training <b><em>(CLIP)</em></b> — to use learnt visual representations linked with natural language.
- Vector-Quantized Generative Adversarial Networks <b><em>(VQ-GAN)</em></b> — to synthesize high quality images.

<br>

<p align="center">
  <img src="./generated_examples/interpolation2.gif" />
  <br><em>Example of Interpolation of generated images with 'Sfumato' art effect generated by the proposed Generative DL architecture in this project</em>
</p>

<br>

<h3>A brief explanation of the approach taken to build the Generative Deep Learning architecture</h3>

We use CLIP to tokenize and encode the provided text prompts — relating to CLIP's learnt visual representations. The process of image generation begins with an initialised noisy image which contians no visual information, which is of pre-set dimensions (user-defined dimensions are 400 pixels X 400 pixels) and is augmented, cropped and (each of the augmented crops are stacked to provide visual context ~ information to CLIP) also tokenized by CLIP. The respective encodings of the image (starting out as noisy) and text prompt are evaluted by Cosine-Similarity — to quantify the similarity between the encodings (i.e. how close they are in terms of representative context) which will help to obtain a performance indicator / Loss of the entire Architecture. The objective in doing this is to ensure that encodings of both text & image match or are as possibly close to each other.
These obtained encodings are then mapped to a learnt Latent Space of the VQ-GAN to generate the required image. Thus, the overall architecture involves having both CLIP & VQ-GAN in a feedback loop where the generated image (which is initialized as noisy) is refined over succcessive iterations — with respective encodings of the generated image & text-prompt become closer to eachother, thus minimizing the evaluated loss.

<h3>Snapshot of different images generated from various sentence prompts</h3>

Here are some of the images generated by the generative architecture presented in this project:

<br>

<p align="center">
  <img src="https://github.com/indropal/GenerativeDeepLearningwithMultimodality/blob/main/generated_examples/GardenofWords.png"/>
  <br><em>"A Garden of Words."</em>
</p>

<br>

<p align="center">
  <img src="https://github.com/indropal/GenerativeDeepLearningwithMultimodality/blob/main/generated_examples/GroupHappySmilingFriends.png"/>
  <br><em>"A group of happy children."</em>
</p>

<br>

<p align="center">
  <img src="https://github.com/indropal/GenerativeDeepLearningwithMultimodality/blob/main/generated_examples/SunnyDayBlueSky.png"/>
  <br><em>"Sunny day with blue sky."</em>
</p>

<br>

<p align="center">
  <img src="https://github.com/indropal/GenerativeDeepLearningwithMultimodality/blob/main/generated_examples/ForestwithPurpleTrees.png"/>
  <br><em>"Forest with purple trees."</em>
</p>
